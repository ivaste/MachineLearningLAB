{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup state \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "# our support vector machine classifier!\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib\n",
    "import numpy\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "#let's ignore python warnings - not a good idea, but simplifies visualization....\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what this dataset is all about\n",
    "print digits.DESCR\n",
    "\n",
    "# The data set contains images of hand-written digits: 10 classes where\n",
    "# each class refers to a digit.\n",
    "\n",
    "# 32x32 bitmaps are divided into nonoverlapping blocks of\n",
    "# 4x4 and the number of on pixels are counted in each block. This generates\n",
    "# an input matrix of 8x8 where each element is an integer in the range\n",
    "# 0..16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what keys this digits object has\n",
    "print digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len( digits.target )\n",
    "print digits.target[:25] # they seem to be in order \n",
    "\n",
    "numpy.histogram( digits.target, bins=10 )[0] # each number is pretty evenly distributed in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what classes we want to put digits into\n",
    "print digits.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pic one and see what it looks like\n",
    "first_digit = digits.data[1300]\n",
    "print first_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what do all of those columns mean!?!?!\n",
    "\n",
    "# let's see what the digit 1300 was tagged as\n",
    "print digits.target[1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see what digits.images[1300] is\n",
    "# it's just the first_digit array reshaped as an 8 x 8 matrix\n",
    "print digits.images[1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image\n",
    "plt.imshow(\n",
    "    digits.images[1300],\n",
    "    cmap          = plt.cm.gray_r,\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many data points do we have?\n",
    "print len( digits.data ) # 1797 data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's create a training set by leaving out the last 1500 of the 1797 data points\n",
    "leftout = 1500\n",
    "x_training, y_training = digits.data[:-leftout], digits.target[:-leftout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define a function to check accuracy for a given classifier on validation data\n",
    "\n",
    "def check_accuracy_validation( classifier, dataset, lower_bound ):\n",
    "    correct = 0\n",
    "    indices = range( lower_bound, 0 )\n",
    "    for i in indices:\n",
    "        # if we were correct\n",
    "        if classifier.predict( dataset.data[i].reshape(1, -1) )[0] == dataset.target[i]:\n",
    "            correct += 1\n",
    "    accuracy = float( correct ) / len( indices )\n",
    "    return accuracy\n",
    "\n",
    "# Let's define a function to check accuracy for a given classifier on training data\n",
    "def check_accuracy_training( classifier, dataset, lower_bound ):\n",
    "    correct = 0\n",
    "    num_samples = dataset.data.shape[0]\n",
    "    indices = range( 0, num_samples+lower_bound )\n",
    "    for i in indices:\n",
    "        # if we were correct\n",
    "        if classifier.predict( dataset.data[i].reshape(1, -1) )[0] == dataset.target[i]:\n",
    "            correct += 1\n",
    "    accuracy = float( correct ) / len( indices )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn SVM uses a parameter $C$ that \"weights\" the error term instead of a parameter $\\lambda$ that weights the $\\ell_2$ regularization terms, so you can think of $C \\approx \\frac{1}{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier = svm.SVC( C=1, kernel='linear' )\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier, digits, -leftout )\n",
    "print \"Linear SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier, digits, -leftout )\n",
    "print \"Linear SVC Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier2 = svm.SVC( gamma=1, C=1, kernel='poly',degree=10 )\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier2.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier2.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier2, digits, -leftout )\n",
    "print \"poly SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier2, digits, -leftout )\n",
    "print \"poly SVC Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier3 = svm.SVC( gamma=1, C=1, kernel='sigmoid')\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier3.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier3.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier3, digits, -leftout )\n",
    "print \"sigmoid SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier3, digits, -leftout )\n",
    "print \"sigmoid SVC Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier4 = svm.SVC( gamma=1, C=100, kernel='rbf')\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier4.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier4.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier4, digits, -leftout )\n",
    "print \"rbf SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier4, digits, -leftout )\n",
    "print \"rbf SVC Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's try with Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit( x_training, y_training )\n",
    "\n",
    "# Let's see what the accuracy is\n",
    "# Let's go from digits.data[-200] all the way to digits.data[-1]\n",
    "accuracy = check_accuracy_training( log_reg, digits, -leftout )\n",
    "print \"Logistic Regression Accuracy on Training Set: {}\".format( accuracy )\n",
    "accuracy = check_accuracy_validation( log_reg, digits, -leftout )\n",
    "print \"Logistic Regression Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What happens to the accuracy when you change gamma in rbf the SVM to .01? .1? 1? almost 0? And C to be 1? 10? 100? 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    for C in [1, 10, 100, 1000]:\n",
    "        classifier = svm.SVC( gamma=gamma, C=C, kernel='rbf' )\n",
    "        classifier.fit( x_training, y_training )\n",
    "        accuracy = check_accuracy_training( classifier, digits, -leftout )\n",
    "        print \"Accuracy on training set for gamma = %f, C = %d: \\t\\t%f\" % ( gamma, C, accuracy )\n",
    "        accuracy = check_accuracy_validation( classifier, digits, -leftout )\n",
    "        print \"Accuracy on validation set for gamma = %f, C = %d: \\t\\t%f\" % ( gamma, C, accuracy )\n",
    "\n",
    "# accuracy decreases as gamma increases; the \"best\" value seems to be around 0.0001 to 0.001\n",
    "# accuracy increases a little as C increases, though not by much and not always (note: C cannot be 0!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to the accuracy when you train the sigmoid SVM or the rbf SVM on many more samples? Let's say all but the last 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout = 300\n",
    "\n",
    "#sigmoid kernel\n",
    "\n",
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier3 = svm.SVC( gamma=1, C=1, kernel='sigmoid')\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier3.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier3.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier3, digits, -leftout )\n",
    "print \"sigmoid SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier3, digits, -leftout )\n",
    "print \"sigmoid SVC Accuracy on Validation Set: {}\".format( accuracy )\n",
    "\n",
    "#RBF kernel\n",
    "\n",
    "# let's instantiate our Support Vector Classifier with linear kernel\n",
    "classifier4 = svm.SVC( gamma=1, C=100, kernel='rbf')\n",
    "# C is penalty parameter of the error term\n",
    "\n",
    "# now let's train the classifier on the training data\n",
    "classifier4.fit( x_training, y_training )\n",
    "\n",
    "print \"Prediction {}\".format( classifier4.predict( digits.data[1300].reshape(1, -1) ) )\n",
    "print digits.target[1300] # it got it right \n",
    "\n",
    "accuracy = check_accuracy_training( classifier4, digits, -leftout )\n",
    "print \"rbf SVC Accuracy on Training Set: {}\".format( accuracy ) \n",
    "\n",
    "accuracy = check_accuracy_validation( classifier4, digits, -leftout )\n",
    "print \"rbf SVC Accuracy on Validation Set: {}\".format( accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
